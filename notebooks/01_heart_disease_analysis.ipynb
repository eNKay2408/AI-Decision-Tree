{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a21277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template Notebook cho Data Preparation - Milestone 1\n",
    "# Thành viên: NGUYỄN VĂN CHIẾN - Dataset: THE UCI HEART DISEASE DATASET\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Import utilities\n",
    "import sys\n",
    "sys.path.append('../src/')\n",
    "from visualization_utils import create_standard_plots, plot_class_distribution, plot_split_distributions\n",
    "\n",
    "# ==========================================\n",
    "# 1. LOAD AND EXPLORE DATA\n",
    "# ==========================================\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"1. LOAD AND EXPLORE DATA\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Load dataset\n",
    "col_names= ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', \n",
    "             'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target']\n",
    "\n",
    "data = pd.read_csv('../datasets/heart_disease/heart.csv', header=None, names=col_names)\n",
    "\n",
    "# Display dataset information\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(\"\\nDataset info:\")\n",
    "print(data.info())\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(data.head())\n",
    "\n",
    "# Check missing values\n",
    "print(\"\\nMissing values:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Check '?' values -> missing data\n",
    "print(\"\\nChecking for '?' values:\")\n",
    "for col in data.columns:\n",
    "    question_marks = (data[col] == '?').sum()\n",
    "    if question_marks > 0:\n",
    "        print(f\"Column '{col}' has {question_marks} '?' values\")\n",
    "\n",
    "# Descriptive statistics\n",
    "print(\"\\nDescriptive statistics:\")\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88733a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2. DATA PREPROCESSING\n",
    "# ==========================================\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"2. DATA PREPROCESSING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Replace '?' with NaN\n",
    "data = data.replace('?', np.nan)\n",
    "\n",
    "# Convert columns to appropriate types\n",
    "for col in ['ca', 'thal']:\n",
    "    data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "\n",
    "# Handle missing values\n",
    "data = data.dropna()\n",
    "\n",
    "# Check class distribution\n",
    "# =============================================\n",
    "# Convert target to int\n",
    "data['target'] = data['target'].astype(int)\n",
    "\n",
    "# Check class distribution\n",
    "print(\"\\nClass distribution:\")\n",
    "print(data['target'].value_counts())\n",
    "print(\"Class proportions:\")\n",
    "print(data['target'].value_counts(normalize=True).mul(100).round(2).astype(str) + '%')\n",
    "\n",
    "# Check target values and their meaning\n",
    "target_values = sorted(data['target'].unique())\n",
    "print(f\"\\nTarget values: {target_values}\")\n",
    "print(\"0: <50% diameter narrowing (no heart disease)\")\n",
    "print(\"1-4: >50% diameter narrowing (heart disease)\")\n",
    "\n",
    "# Simplify target to binary classification if needed\n",
    "data['target_binary'] = data['target'].apply(lambda x: 0 if x == 0 else 1)\n",
    "print(\"\\nBinary target distribution:\")\n",
    "print(data['target_binary'].value_counts())\n",
    "print(\"Binary class proportions:\")\n",
    "print(data['target_binary'].value_counts(normalize=True).mul(100).round(2).astype(str) + '%')\n",
    "# ============================================\n",
    "\n",
    "# Prepare features and target\n",
    "X = data.drop(['target', 'target_binary'], axis=1)\n",
    "y = data['target_binary']\n",
    "\n",
    "print(\"Features shape:\", X.shape)\n",
    "print(\"Target shape:\", y.shape)\n",
    "\n",
    "print(f\"Class balance ratio (negative:positive): {(y==0).sum()}/{(y==1).sum()} = {((y==0).sum()/(y==1).sum()):.2f}\")\n",
    "\n",
    "# Add to EDA section\n",
    "print(\"Feature correlation with target:\")\n",
    "print(data.corr()['target_binary'].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bc0d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3. VISUALIZE CLASS DISTRIBUTION\n",
    "# ==========================================\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"3. VISUALIZE CLASS DISTRIBUTION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "dataset_name = \"Heart Disease\"\n",
    "target_col = \"target_binary\"\n",
    "\n",
    "plot_class_distribution(data, target_col, f'{str(dataset_name)} - Original Data',\n",
    "                      f'../visualizations/heart_disease/original_distribution.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1997a88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 4. SPLIT DATA INTO 4 DIFFERENT RATIOS\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"4. SPLIT DATA INTO 4 DIFFERENT RATIOS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Define split ratios\n",
    "split_ratios = {\n",
    "    '40/60': 0.4,\n",
    "    '60/40': 0.6, \n",
    "    '80/20': 0.8,\n",
    "    '90/10': 0.9\n",
    "}\n",
    "\n",
    "# Dictionary to store splits\n",
    "data_splits = {}\n",
    "target_splits = {}\n",
    "\n",
    "# Perform stratified splits for each ratio\n",
    "for ratio_name, train_size in split_ratios.items():\n",
    "    print(f\"\\nSplitting data with ratio {ratio_name}...\")\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        train_size=train_size,\n",
    "        stratify=y,  # Ensure stratified split\n",
    "        random_state=42,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    # Store in dictionary\n",
    "    data_splits[ratio_name] = (X_train, X_test)\n",
    "    target_splits[ratio_name] = (y_train, y_test)\n",
    "    \n",
    "    # Print split information\n",
    "    print(f\"Train size: {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "    print(f\"Test size: {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "    print(f\"Train class distribution:\\n{y_train.value_counts()}\")\n",
    "    print(f\"Test class distribution:\\n{y_test.value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58b6278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 5. VISUALIZE SPLIT DISTRIBUTIONS  \n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"5. VISUALIZE SPLIT DISTRIBUTIONS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Prepare data for visualization\n",
    "splits_for_viz = {}\n",
    "for ratio_name in split_ratios.keys():\n",
    "    y_train, y_test = target_splits[ratio_name]\n",
    "    splits_for_viz[ratio_name] = (y_train, y_test)\n",
    "\n",
    "# Plot distribution comparisons\n",
    "plot_split_distributions(splits_for_viz, dataset_name, \n",
    "                        f'../visualizations/heart_disease/split_distributions.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc87c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 6. SAVE RESULTS\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"6. SAVE RESULTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create output directory\n",
    "import os\n",
    "output_dir = '../results/heart_disease'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save splits for Milestone 2\n",
    "import pickle\n",
    "\n",
    "with open(f'{output_dir}/data_splits.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'data_splits': data_splits,\n",
    "        'target_splits': target_splits,\n",
    "        'original_data': (X, y),\n",
    "        'dataset_info': {\n",
    "            'name': dataset_name,\n",
    "            'shape': data.shape,\n",
    "            'features': list(X.columns),\n",
    "            'target': target_col,\n",
    "            'classes': sorted(y.unique().tolist())\n",
    "        }\n",
    "    }, f)\n",
    "\n",
    "print(f\"✅ Data splits saved to {output_dir}/data_splits.pkl\")\n",
    "\n",
    "# Create summary report\n",
    "summary_stats = []\n",
    "for ratio_name in split_ratios.keys():\n",
    "    y_train, y_test = target_splits[ratio_name]\n",
    "    summary_stats.append({\n",
    "        'Split Ratio': ratio_name,\n",
    "        'Train Size': len(y_train),\n",
    "        'Test Size': len(y_test),\n",
    "        'Train Class 0': (y_train == 0).sum(),\n",
    "        'Train Class 1': (y_train == 1).sum(),\n",
    "        'Test Class 0': (y_test == 0).sum(), \n",
    "        'Test Class 1': (y_test == 1).sum()\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_stats)\n",
    "summary_df.to_csv(f'{output_dir}/split_summary.csv', index=False)\n",
    "print(f\"✅ Summary saved to {output_dir}/split_summary.csv\")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\nSplit Summary:\")\n",
    "print(summary_df)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"COMPLETED ✅\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Dataset: {dataset_name}\")\n",
    "print(f\"Original samples: {len(data)}\")\n",
    "print(f\"Features: {X.shape[1]}\")\n",
    "print(f\"Classes: {len(y.unique())}\")\n",
    "print(f\"Splits created: {len(split_ratios)}\")\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8a74fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# MILESTONE 2: DECISION TREE CLASSIFIERS\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BULD AND EVALUATE DECISION TREE CLASSIFIERS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Import thêm các thư viện cần thiết cho Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_graphviz\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Tạo thư mục lưu trữ kết quả về tree\n",
    "viz_tree_dir = '../visualizations/heart_disease/trees'\n",
    "os.makedirs(viz_tree_dir, exist_ok=True)\n",
    "\n",
    "# Tạo hàm trực quan hóa cây quyết định\n",
    "def visualize_tree_sklearn(model, feature_names, class_names, file_name, max_depth=None, sub_file_name=None):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plot_tree(model, \n",
    "              feature_names=feature_names,  \n",
    "              class_names=class_names,\n",
    "              filled=True, \n",
    "              rounded=True,\n",
    "              max_depth=max_depth,\n",
    "              fontsize=10,\n",
    "              precision=4)\n",
    "    \n",
    "     # Xử lý tên file - thay thế ký tự / bằng _\n",
    "    safe_file_name = file_name.replace('/', '_')\n",
    "\n",
    "    plt.title(f'Decision Tree - {file_name}')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Tạo thư mục nếu chưa tồn tại\n",
    "    os.makedirs(viz_tree_dir, exist_ok=True)\n",
    "    \n",
    "    # Lưu hình ảnh\n",
    "    file_path = f\"{viz_tree_dir}/{sub_file_name}/{safe_file_name}.png\"\n",
    "    plt.savefig(file_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"✅ Decision Tree has been saved at: {file_path}\")\n",
    "    return file_path\n",
    "\n",
    "# ==========================================\n",
    "# 2.2. BUILD DECISION TREE CLASSIFIERS\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"2.2. BUILD DECISION TREE CLASSIFIERS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Dictionary để lưu các mô hình\n",
    "models = {}\n",
    "\n",
    "# Xây dựng và trực quan hóa các Decision Tree cho mỗi tỉ lệ train/test\n",
    "for split_name in data_splits:\n",
    "    print(f\"\\nModeling for {split_name} ratio:\")\n",
    "    X_train, X_test = data_splits[split_name]\n",
    "    y_train, y_test = target_splits[split_name]\n",
    "    \n",
    "    # Khởi tạo và huấn luyện mô hình Decision Tree\n",
    "    # Sử dụng criterion='entropy' để sử dụng information gain\n",
    "    model = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    models[split_name] = model\n",
    "    \n",
    "    # Thông tin về mô hình\n",
    "    print(f\"Number of nodes: {model.tree_.node_count}\")\n",
    "    print(f\"Depth of tree: {model.get_depth()}\")\n",
    "    \n",
    "    # Trực quan hóa decision tree\n",
    "    visualize_tree_sklearn(model, \n",
    "                          feature_names=X_train.columns, \n",
    "                          class_names=['No Disease', 'Disease'], \n",
    "                          file_name=f\"{split_name}_tree\", sub_file_name='build_tree')\n",
    "\n",
    "# Lưu các mô hình\n",
    "with open(f'{output_dir}/tree_models.pkl', 'wb') as f:\n",
    "    pickle.dump(models, f)\n",
    "    \n",
    "print(f\"\\n✅ All Models have been save at: {output_dir}/tree_models.pkl\")\n",
    "\n",
    "# Hiển thị hình ảnh cây quyết định cho tỉ lệ 80/20\n",
    "try:\n",
    "    from IPython.display import Image\n",
    "    Image(filename=f\"{viz_tree_dir}/80/20_tree.png\")\n",
    "except:\n",
    "    print(\"Cannot dislay image directly. View at file:\", f\"{viz_tree_dir}/80/20_tree.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcb56e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2.3. ĐÁNH GIÁ DECISION TREE CLASSIFIERS\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"2.3. EVALUATE DECISION TREE CLASSIFIERS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Dictionary để lưu kết quả đánh giá\n",
    "evaluation_results = {}\n",
    "\n",
    "# Đánh giá từng mô hình\n",
    "for split_name in models:\n",
    "    print(f\"\\n{'-'*30}\")\n",
    "    print(f\"Model evaluation for {split_name} ratio:\")\n",
    "    print(f\"{'-'*30}\")\n",
    "    \n",
    "    model = models[split_name]\n",
    "    X_train, X_test = data_splits[split_name]\n",
    "    y_train, y_test = target_splits[split_name]\n",
    "    \n",
    "    # Dự đoán trên tập test\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Tính toán các metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    class_report = classification_report(y_test, y_pred, target_names=['No Disease', 'Disease'])\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Lưu kết quả\n",
    "    evaluation_results[split_name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'classification_report': class_report,\n",
    "        'confusion_matrix': conf_matrix\n",
    "    }\n",
    "    \n",
    "    # In kết quả đánh giá\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(class_report)\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "    \n",
    "    # Vẽ và lưu confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "               xticklabels=['No Disease', 'Disease'],\n",
    "               yticklabels=['No Disease', 'Disease'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(f\"Confusion Matrix - Split {split_name}\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Xử lý tên file - thay thế ký tự / bằng _\n",
    "    safe_file_name = f\"{split_name.replace('/', '_')}_confusion_matrix.png\"\n",
    "\n",
    "    plt.savefig(f\"{viz_tree_dir}/assess_tree/{safe_file_name}\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"✅ Confusion Matrix has been saved at: {viz_tree_dir}/assess_tree/{safe_file_name}\")\n",
    "\n",
    "# Lưu kết quả đánh giá\n",
    "with open(f'{output_dir}/evaluation_results.pkl', 'wb') as f:\n",
    "    pickle.dump(evaluation_results, f)\n",
    "\n",
    "print(f\"\\n✅ All evaluation results have been saved at: {output_dir}/evaluation_results.pkl\")\n",
    "\n",
    "# Phân tích và diễn giải các kết quả\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ANALYZE AND INTERPRETE THE RESULTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\"\"\n",
    "Classification Report Analysis:\n",
    "- Precision: The ratio of correctly predicted positive observations to the total predicted positives. It represents how many of the samples identified as positive are actually positive.\n",
    "- Recall: The ratio of correctly predicted positive observations to all actual positives in the dataset. It indicates how many of the actual positive samples were correctly identified.\n",
    "- F1-score: The harmonic mean of Precision and Recall, providing a balance between the two metrics.\n",
    "- Support: The number of actual occurrences of each class in the dataset.\n",
    "\n",
    "Confusion Matrix Analysis:\n",
    "- True Negative (TN): Number of patients without heart disease who were correctly classified as not having the disease.\n",
    "- False Positive (FP): Number of patients without heart disease who were incorrectly classified as having the disease.\n",
    "- False Negative (FN): Number of patients with heart disease who were incorrectly classified as not having the disease.\n",
    "- True Positive (TP): Number of patients with heart disease who were correctly classified as having the disease.\n",
    "\n",
    "\n",
    "Conclusion:\n",
    "In the context of heart disease diagnosis, False Negatives (missing patients with disease) are more dangerous than False Positives (incorrectly diagnosing healthy individuals), therefore the Recall metric for the \"Disease\" class is particularly critical.\n",
    "\"\"\")\n",
    "\n",
    "# Tạo bảng so sánh accuracy giữa các tỉ lệ\n",
    "accuracies = {split_name: results['accuracy'] for split_name, results in evaluation_results.items()}\n",
    "accuracy_df = pd.DataFrame(list(accuracies.items()), columns=['Split Ratio', 'Accuracy'])\n",
    "accuracy_df = accuracy_df.sort_values('Split Ratio')\n",
    "\n",
    "print(\"\\nAccuracy comparison between train/test ratios:\")\n",
    "print(accuracy_df)\n",
    "\n",
    "# Vẽ biểu đồ so sánh accuracy\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(accuracy_df['Split Ratio'], accuracy_df['Accuracy'], color='skyblue')\n",
    "plt.xlabel('Train/Test Ratio')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Compare accuracy between Train/Test ratios')\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for i, v in enumerate(accuracy_df['Accuracy']):\n",
    "    plt.text(i, v + 0.01, f'{v:.4f}', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{viz_tree_dir}/comparison/accuracy_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"✅ Visualization of accuracy comparison has been saved at: {viz_tree_dir}/accuracy_comparison.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afe7931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2.4. PHÂN TÍCH ẢNH HƯỞNG CỦA ĐỘ SÂU CÂY\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"2.4. ANALYSIS OF THE IMPACT OF PLANT DEPTH\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Sử dụng tỉ lệ 80/20 theo yêu cầu\n",
    "X_train, X_test = data_splits['80/20']\n",
    "y_train, y_test = target_splits['80/20']\n",
    "\n",
    "# Danh sách các giá trị max_depth cần thử nghiệm\n",
    "max_depths = [None, 2, 3, 4, 5, 6, 7]\n",
    "depth_results = {}\n",
    "\n",
    "print(\"\\nExperiment with different max_depth values ​​(80/20 ratio):\")\n",
    "for depth in max_depths:\n",
    "    # Xây dựng mô hình với độ sâu được chỉ định\n",
    "    model = DecisionTreeClassifier(criterion='entropy', max_depth=depth, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Dự đoán và tính accuracy\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    depth_results[depth] = accuracy\n",
    "    \n",
    "    depth_str = str(depth) if depth is not None else 'None'\n",
    "    print(f\"Max Depth: {depth_str}, Accuracy: {accuracy:.4f}, Number of nodes: {model.tree_.node_count}\")\n",
    "    \n",
    "    # Trực quan hóa cây quyết định\n",
    "    visualize_tree_sklearn(model, \n",
    "                          feature_names=X_train.columns, \n",
    "                          class_names=['No Disease', 'Disease'], \n",
    "                          file_name=f\"depth_{depth_str}\",\n",
    "                          max_depth=depth, sub_file_name='depth_analysis')\n",
    "\n",
    "# Vẽ biểu đồ độ chính xác theo độ sâu\n",
    "plt.figure(figsize=(10, 6))\n",
    "depths_for_plot = [7 if d is None else d for d in max_depths]  # None sẽ được biểu diễn như độ sâu tối đa\n",
    "labels = [str(d) if d is not None else 'None' for d in max_depths]\n",
    "\n",
    "plt.plot(depths_for_plot, list(depth_results.values()), marker='o', linestyle='-', linewidth=2, markersize=8)\n",
    "plt.xticks(depths_for_plot, labels)\n",
    "plt.xlabel('Max_depth value')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Effect of max_depth on the accuracy of Decision Tree')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(f\"{viz_tree_dir}/depth_accuracy/depth_vs_accuracy.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(f\"✅ Visualizations of the effect of max_depth has been saved at: {viz_tree_dir}/depth_accuracy/depth_vs_accuracy.png\")\n",
    "\n",
    "# Tạo bảng kết quả\n",
    "col_width = 10\n",
    "\n",
    "# Tạo header của bảng\n",
    "header = \"| \" + \"max_depth\".center(col_width) + \" |\"\n",
    "for d in max_depths:\n",
    "    val = \"None\" if d is None else str(d)\n",
    "    header += \" \" + val.center(col_width) + \" |\"\n",
    "print(header)\n",
    "\n",
    "# Tạo dòng phân cách\n",
    "separator = \"| \" + \"-\" * col_width + \" |\"\n",
    "for _ in max_depths:\n",
    "    separator += \" \" + \"-\" * col_width + \" |\"\n",
    "print(separator)\n",
    "\n",
    "# Tạo dòng accuracy\n",
    "accuracy_row = \"| \" + \"Accuracy\".center(col_width) + \" |\"\n",
    "for d in max_depths:\n",
    "    val = f\"{depth_results[d]:.4f}\"\n",
    "    accuracy_row += \" \" + val.center(col_width) + \" |\"\n",
    "print(accuracy_row)\n",
    "\n",
    "# Lưu kết quả phân tích độ sâu\n",
    "with open(f'{output_dir}/depth_analysis.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'max_depths': max_depths,\n",
    "        'accuracies': depth_results\n",
    "    }, f)\n",
    "\n",
    "print(f\"\\n✅ The depth analysis results have been saved at: {output_dir}/depth_analysis.pkl\")\n",
    "\n",
    "# ==========================================\n",
    "# INSIGHTS VÀ MỘT SỐ KẾT LUẬN\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"INSIGHTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\"\"\n",
    "From our analysis of building and evaluating Decision Tree models for the Heart Disease dataset, we draw the following conclusions:\n",
    "\n",
    "1. Performance Across Different Train/Test Ratios:\n",
    "   - The train/test ratio significantly impacts model performance\n",
    "   - Ratios with more training data generally yield better results\n",
    "   - However, with limited samples, there needs to be careful consideration between training data volume and testing data adequacy\n",
    "\n",
    "2. Impact of Tree Depth:\n",
    "   - Low max_depth (2-3): Models are simple and interpretable but may lack capacity to learn complex patterns\n",
    "   - Medium max_depth (4-6): Models can capture more complex relationships in the data\n",
    "   - High max_depth or unlimited: Risk of overfitting increases as the model \"memorizes\" the training data\n",
    "   - Finding the optimal max_depth value is crucial to balance between model complexity and generalization ability\n",
    "\n",
    "3. Heart Disease Prediction Considerations:\n",
    "   - Decision Trees are valuable tools for medical diagnosis due to their interpretability\n",
    "   - Special attention must be paid to the Recall metric for the \"Disease\" class to minimize missed disease cases\n",
    "   - Identifying optimal classification thresholds (as shown in Figure 1 of the assignment) helps improve diagnostic accuracy\n",
    "      \n",
    "4. Additional Insights:\n",
    "   - Feature importance analysis reveals which medical indicators are most critical for diagnosis\n",
    "   - The visualized decision trees provide medical professionals with interpretable decision paths\n",
    "   - While accuracy is important, in medical contexts like heart disease diagnosis, the consequences of different types of errors (false positives vs. false negatives) must be carefully weighted\n",
    "   - For real-world applications, more sophisticated tree-based methods like Random Forests or Gradient Boosting might provide better performance while maintaining some level of interpretability\n",
    "      \n",
    "5. Future Work:\n",
    "   - Experiment with different hyperparameter combinations beyond just tree depth\n",
    "   - Consider feature engineering to improve model performance\n",
    "   - Explore class weighting to address the importance of correctly identifying disease cases\n",
    "   - Investigate pruning techniques to further optimize tree structure and prevent overfitting\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (AI Decision Tree)",
   "language": "python",
   "name": "ai-decision-tree-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
