{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98806805",
   "metadata": {},
   "source": [
    "# Decision Tree Depth Analysis on 80/20 Split\n",
    "\n",
    "In this section, we:\n",
    "\n",
    "- Focus on the 80/20 split to analyze how the maximum depth of the decision tree impacts model accuracy.\n",
    "- Experiment with various `max_depth` values to explore the trade-off between model complexity and performance.\n",
    "- Visualize the tree models with various `max_depth` parameters using Graphviz \n",
    "- Summarize the results in both a table and a plot to clearly illustrate the relationship between tree depth and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef78a574",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run 03_additional_dataset_preparation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01e7ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import graphviz\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a86794e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define different max_depth values to test\n",
    "max_depths = [None, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "# Store accuracy results for comparison\n",
    "depth_accuracies = []\n",
    "\n",
    "# Focus on 80/20 split for depth analysis\n",
    "X_train, X_test = feature_train_80_20, feature_test_80_20\n",
    "y_train, y_test = label_train_80_20, label_test_80_20\n",
    "\n",
    "print(\"=== Decision Tree Depth Analysis on 80/20 Split ===\\n\")\n",
    "\n",
    "for max_depth in max_depths:\n",
    "    print(f\"\\n--- Max Depth: {max_depth} ---\")\n",
    "    \n",
    "    # Create and train the decision tree\n",
    "    clf = DecisionTreeClassifier(criterion='entropy', max_depth=max_depth, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    depth_accuracies.append((max_depth, accuracy))\n",
    "    \n",
    "    # Print classification report\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Visualize the tree using Graphviz\n",
    "    dot_data = export_graphviz(\n",
    "        clf, out_file=None, \n",
    "        feature_names=X_train.columns,\n",
    "        class_names=[str(cls) for cls in sorted(y_train.unique())],\n",
    "        filled=True, rounded=True, special_characters=True\n",
    "    )\n",
    "    graph = graphviz.Source(dot_data)\n",
    "    print(f\"\\nDecision Tree Visualization (max_depth={max_depth}):\")\n",
    "    display(graph)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80) # print the seperator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c5a604",
   "metadata": {},
   "outputs": [],
   "source": [
    "depths, accuracies = zip(*depth_accuracies)\n",
    "depths_str = ['None' if d is None else str(d) for d in depths]\n",
    "\n",
    "# Print the accuracy for each max depth table\n",
    "print(\"Max Depth vs Accuracy Table\")\n",
    "table_data = [['max_depth'] + depths_str, ['Accuracy'] + list(accuracies)]\n",
    "print(tabulate(table_data, headers='firstrow', tablefmt='fancy_grid'))\n",
    "\n",
    "# Plot accuracy comparison across different max_depth values with combined bar and line chart\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "# Create bar chart\n",
    "bars = plt.bar(range(len(depths)), accuracies, alpha=0.6, color='lightblue', \n",
    "               edgecolor='navy', linewidth=1.5, label='Accuracy')\n",
    "\n",
    "# Add line plot on top\n",
    "plt.plot(range(len(depths)), accuracies, linewidth=3, markersize=10, \n",
    "         color='red', markerfacecolor='darkred', label='Trend Line')\n",
    "\n",
    "# Customize the plot\n",
    "plt.xticks(range(len(depths)), depths_str, fontsize=11)\n",
    "plt.xlabel('Maximum Depth', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "plt.title('Decision Tree Accuracy vs Maximum Depth (80/20 Split)', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.grid(True, alpha=0.3, linestyle='--')\n",
    "\n",
    "# Set y-axis to show full range of accuracy values with some padding\n",
    "min_acc = min(accuracies)\n",
    "max_acc = max(accuracies)\n",
    "padding = (max_acc - min_acc) * 0.1  # 10% padding\n",
    "plt.ylim(max(0, min_acc - padding), min(1.0, max_acc + padding))\n",
    "\n",
    "# Add accuracy values on top of bars\n",
    "for i, acc in enumerate(accuracies):\n",
    "    plt.annotate(f'{acc:.4f}', (i, acc), textcoords=\"offset points\", \n",
    "                xytext=(0,12), ha='center', fontsize=10, fontweight='bold',\n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"yellow\", alpha=0.7))\n",
    "\n",
    "# Add legend\n",
    "plt.legend(loc='upper right', fontsize=11)\n",
    "\n",
    "# Highlight the best performing depth\n",
    "best_idx = accuracies.index(max(accuracies))\n",
    "plt.annotate(f'Best: {max(accuracies):.4f}', \n",
    "             xy=(best_idx, max(accuracies)), \n",
    "             xytext=(best_idx + 0.5, max(accuracies) + 0.02),\n",
    "             arrowprops=dict(arrowstyle='->', color='green', lw=2),\n",
    "             fontsize=12, fontweight='bold', color='green',\n",
    "             bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgreen\", alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e9fbe8",
   "metadata": {},
   "source": [
    "## Insights on the statistics reported\n",
    "\n",
    "The model’s accuracy shows a clear positive correlation with the maximum depth of the decision tree. \n",
    "\n",
    "At a shallow depth of 2, the accuracy is only around 0.47—marginally better than random guessing across six classes (which would yield roughly 16.7%) but still far from sufficient. This highlights how limited model capacity at low depth fails to capture the complexity of the data, especially for a task as sensitive as medical diagnostics.\n",
    "\n",
    "As the depth increases, accuracy improves steadily, ultimately reaching 0.91 when no depth limit is imposed. However, the model’s performance reaches a plateau between depths 4 and 5, where the accuracy remains unchanged at 0.6216, which suggests that the additional layer contributes little to improving prediction quality—possibly resolving only marginal patterns that don’t significantly influence overall outcomes.\n",
    "\n",
    "Beyond this plateau, accuracy continues to improve with depth, indicating that additional layers enable the model to form more refined and specific decision boundaries. Given that the dataset contains over 30 input features and considerable class overlap, deeper trees likely help disentangle complex relationships between features and classes.\n",
    "\n",
    "Overall, the trend indicates that deeper trees are better suited for this classification task, though care must be taken to balance performance gains with the risk of overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
