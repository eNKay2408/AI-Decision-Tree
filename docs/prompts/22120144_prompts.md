# DATA PREPARATION

- I want to do the "Additional Dataset" part, please inspect the UC Irvine Archieve page to find out another dataset that is like this, have many features, contains both features and labels, include at least 300 samples, have multiple classes, there may be Missing value
  _attached the requirements pdf file_

- Please give me detailed step-by-step code for do sing steps 1,2,3 (please follow the provided PDF file as much as possible when it comes to dividing the data into subsets); for step 4, do the visualization of the data itself like specified how should i come about imputation of this dataset? It seems like there is no Exploratory Data Analysis in the requirements? How about some crude ones (for comments/ drawing insights/ comparison), please; please do all of this in Python Notebook (.ipynb), or just provide the code so that I can paste in the notebook

- please update #file:03_additional_dataset_analysis.ipynb so the age imputation is class-wise imputation (group by class, then replace the missing values by median within the class' distribution), and also adding the notes stating it fits, like "robust to outlier, simple, yet ensure contextual awareness";

- please add script that visualize the age histograms for each classes

- upon inspecting the age histogram for each class, i see that the columns may reach very high for some age ranges, like for class 5, the group 15-20 is the most, and it should be the value for the missing if the record is of class 5; currently my class-wise median satisfies this? Or do I have to replace it with the one with the most frequency?

- for categorical variables, like age, i should take mode, right? Since there are no ranks inherently to the age? Or would it be overfitting?

- it seems that the better alternative would to use a more sophisticated imputation method, please update #file:03_additional_dataset_preparation.ipynb , the imputation part, to use scikit-learn iterative imputation; this is definitely better than straight-off mode/ median, right? Since there might be relationship between age and other variables in each classes

- I see that in the requirment file, i'm asked to split the dataset into 4 subset "feature_train: a set of training samples.
  • label_train: a set of labels corresponding to the samples in feature_train.
  • feature_test: a set of test samples with a structure to feature_train.
  • label_test: a set of labels corresponding to the samples in feature_test"
  Please explain these in details, what are their purpose, why do we need them, how do we make it and make sure that we do it right? WHat if we don't approach like this?
  #file:03_additional_dataset_analysis.ipynb #file:AI-Project-2.pdf
  --> answer:

"## Why Split the Dataset into feature_train, label_train, feature_test, and label_test?

When building and evaluating machine learning models, it is essential to split the dataset into four subsets:

- **feature_train**: The input features (all columns except the label) used to train the model.
- **label_train**: The target labels (the class or value to predict) corresponding to each sample in feature_train.
- **feature_test**: The input features for the test set, structured identically to feature_train, but not used during training.
- **label_test**: The true labels for the test set, used to evaluate the model's performance.

### Purpose and Importance

- **Generalization**: Splitting ensures that the model is evaluated on data it has never seen before, providing a realistic estimate of its performance on new, unseen data.
- **Avoiding Overfitting**: If we train and test on the same data, the model may simply memorize the answers (overfit), leading to poor performance on real-world data.
- **Fair Comparison**: Using a separate test set allows us to compare different models or algorithms fairly.

### How to Split Correctly

- Use functions like `train_test_split` from scikit-learn, which can split both features and labels at the same time, ensuring that each sample's features and label stay together.
- Use the `stratify` parameter (for classification) to maintain the same class distribution in both train and test sets.
- Set a `random_state` for reproducibility.

### What If We Don't Split This Way?

- **Testing on Training Data**: The model may appear to perform very well, but this is misleading, as it has already seen the answers.
- **Data Leakage**: Information from the test set could leak into the training process, invalidating the evaluation.
- **Unreliable Results**: The model's reported accuracy or other metrics will not reflect its true ability to generalize to new data.

**In summary:** Properly splitting the dataset into feature_train, label_train, feature_test, and label_test is a fundamental best practice in machine learning to ensure valid, reliable, and generalizable results."

- So the label_train and label_test is for checking, the former is during the trainning session (telling the model 'hey you're wrong please adjust the hidden params, the weights, etc'), while the latter is during the test ('hey you're wrong but this will be counted towards accuracy score, no chance of rebuilding') right?

- what does "split in a stratified fashion" mean? How is it different from ordinary or other splitting methods? How does it affect the data, the distribution, .etc?

- so in other words, to satisfy the requirements, from the original data, i need to make it into 4 subsets 4-6, 6-4, 8-2, 9-1 in terms of train-test; then for each subset, i seperate into feature_train, label_train, feature_set, label_test? And "stratified" here means that in the initial 4 subsets divided based on train-test ratio, there won't be any one same record that any 2 subsets share, but it must be ensure that the class distribution across them is similar? Does it need to go as far as being similar to the original whole data set ?

according to this from the file, "his task sets up the training and test datasets for the upcoming experiments.
Using the features and labels above, please prepare the following four subsets:
• feature_train: a set of training samples.
• label_train: a set of labels corresponding to the samples in feature_train.
• feature_test: a set of test samples with a structure to feature_train.
• label_test: a set of labels corresponding to the samples in feature_test.
You need to shuffle the dataset before splitting and ensure it is split in a stratified fashion.
Other parameters (if there are any) should remain at their default settings.
There will be experiments on training and test sets with different proportions, including 40/60,
60/40, 80/20, and 90/10 (train/test); therefore, you will need 16 subsets in total.
Visualize the class distributions in all datasets (the original set, training sets, and test sets)
across all proportions to demonstrate that they have been appropriately prepared"
I first split into feature_train, label_train, .etc, then I split train-test; but I can do the other way around, right? It seems a bit more intuitive, at first we should try to make sure 4 subsets (each with different train-test ratio) possess the same class distribution first, then we can simply take out \_train, \_test chunk correspondingly

please perform the suggested workflow, make sure to save the subsets into 16 subsets (16 variables), again, I think this is required, right, based on the text

please add in the visualization; if it is stratified, the 4 visualization (after intial stratifed split, not the \_train, \_test split) should share common traits with that of the original dataset when it comes to class distribution, right ?

I see in some graphs, the train (blue color) is more than the general (the result of merging train and test color?), right? And sometimes it's the test that is more. Only by a negligible margin, though, this is acceptable, and still satisfies "stratified split", right?
--> answer:
Yes, you are correct! In stratified splitting, the proportions of each class in the train and test sets are made as close as possible to the original dataset, but due to rounding and the discrete nature of splitting (especially with small class counts), there can be very small differences.

- Sometimes the train set may have a slightly higher or lower proportion for a class than the original.
  The same can happen for the test set.
  When you merge train and test, you recover the original distribution exactly.
  These negligible differences are normal and acceptable. As long as the class proportions in train and test are very close to the original, your split is considered stratified and satisfies the requirement.

- is there anything in #file:visualization_utils.py that i can use to help visualize? Like less repetitive on my file; also please use the method generate_summary_table to generate the table, the save path should be in results/additional_dataset/data_split_result ?

- now please update that the class imbalance ratio is printed before the visualization; and it's for each class (6 classes in my case); would it happen to have any type of graphs suited to display class imbalance/ ratios like this?
  --> answer: The notebook now prints the class counts and imbalance ratio for all 6 classes before any visualization. Additionally, a pie chart has been added alongside the bar chart to visually display the class imbalance and proportions. Both bar and pie charts are commonly used to illustrate class imbalance in classification datasets.

- does this have one-hot encoding? #file:03_additional_dataset_preparation.ipynb, please apply one hot encoding

===============================================================================================================================================================

# CLASSIFICATION MODELS AND EVALUATION SOURCE CODE

- please modify the #file:03_additional_dataset_models_evaluation.ipynb to use the data prepared in #file:03_additional_dataset_preparation.ipynb , then conducting experiments on the designated train/test proportions prepared by fitting an instance of sklearn.tree.DecisionTreeClassifier (using information gain)
  to each training set and visualize the resulting decision tree with Graphviz, please also add in a brief introduction comment at the start to tell what we will be doing in this file.
  For each of the above decision tree classifiers, predict the samples in the corresponding test set
  and generate a report using classification_report and confusion_matrix

- the confustion matrix is just a bland table, can it be made to be colorful, my lecturer's example is one with four sector, each with its own color (each vary in intensity), names, while the vertical ruler on the right exlain the association between the numbers and the color tone
  -> answer: The code now includes a colorful confusion matrix visualization after the text-based output. It uses seaborn's heatmap to display each sector with varying color intensity, class names on both axes, and a vertical colorbar on the right that explains the association between the numbers and the color tone. This should closely match your lecturer's example and make the confusion matrix much more visually informative.

# 2.3 EVALUATION

- after building my models in python using sklearn library, what is classification report and confusion matrix? How to interpret them? What information can i derive from those? In terms of decision tree performance, what can be drawn out from those? How the two differs?

- please review and polish this for my document
  "Classification report interpretation (the same for 4 all models)
  A classification report is a summary of performance metrics for each class:
  • Precision: Percentage of correct positive predictions relative to total positive predictions
  • Recall: Percentage of correct positive predictions relative to total actual positives.
  • F1-Score: A weighted harmonic mean of precision and recall. The closer to 1, the better the model, with F1 being 2 _ (Precision _ Recall) / (Precision + Recall)
  • Support: the number of actual samples in the test set when evaluating the model.
  For the overall information, it typically includes:
  Accuracy: The rate of correct prediction the model has made.
  Macro avg: The unweighted average per label for each metric (Precision, Recall, F1-Score, Support)
  Weighted avg: The weighted average per label for each metric (all metrics except for Support)

Confusion matrix interpretation (the same for all models)
A confusion matrix is a n-by-n grid that shows the performance of the model by comparing its predicted labels with the true labels, with n being the number of classes. It presents the number of true positives, true negatives, false positives and false negatives the model has made for each class, with ‘positive’ and ‘negative’ referring to the target class.
• True positive: this is when the model predicts the instance to be of the target class and it is indeed.
• True negative: this is when the model predicts the instance to not belong to the target class and it is not.
• False positive: this is when the model predicts the instance to be of the target class although it is not.
• False negative: this is when the model predicts the instance to not belong to the target class even though it actually is.
"

- Given these "Classification Report for (40/60 split):
  precision recall f1-score support

             1       0.89      0.82      0.85        67
             2       0.56      0.89      0.69        37
             3       1.00      0.91      0.95        43
             4       0.92      0.80      0.86        30
             5       0.68      0.61      0.64        31
             6       1.00      0.50      0.67        12

      accuracy                           0.80       220

macro avg 0.84 0.76 0.78 220
weighted avg 0.84 0.80 0.81 220"
"Crude Confusion matrix for (40/60 split):
[[55  7  0  0  5  0]
 [ 2 33  0  0  2  0]
 [ 0  2 39  1  1  0]
 [ 0  5  0 24  1  0]
 [ 5  6  0  1 19  0]
 [ 0  6  0  0  0  6]]"
Please give me paragraphs providing insigths and comments about the model performance

- a shorter, grouping together classes that do well, class that do badly, and classes that seems to confuse the model (based on the confusion matrix)

- write a same concise passage, but don't make headings and bullet points, instead just small paragraphs

# TREE DEPTH ANALYSIS SOURCE CODE

- please add in the comment saying this part wwe will be focusing on 80/20 test set, and to consider how the depth of the tree affect the tree accuracy; then add in the code that follows:
  For each max_depth in max_depths (max_depths = [None, 2, 3, 4, 5, 6, 7]):
  run the classification tree model on the 80-20 set, and use Graphviz to visualize the tree, then extract out the accuracy metric in the classification_report() method, if possible, if not, just print out classification_report();
  the models, the data has been preped in #03_additional_dataset_preparation.ipynb and #03_additional_dataset_models_evaluation.ipynb

- in the visualization cell, please add in the code to output a plain table of accuracy vs tree_Depth that goes
  max_depth| None|1|2|3|4|5|6|7|
  Accuracy| | | | | | | | | |
  and fill in the accuracy

- utitlize the tabulate as referenced from https://learnpython.com/blog/print-table-in-python/ to make the tabular visualization

- please add in the plot to also have the columns, i mean, to make the plot not just point with line, but also has vertical bars like bar graph, to be more visually appealing

# 2.4 INSIGHTS FROM DEPTH-ACCURACY TABLE

- based on this maximum tree depth and accuracy table Max Depth vs Accuracy Table
  ╒═════════════╤══════════╤══════════╤══════════╤══════════╤══════════╤══════════╤═════════╕
  │ max_depth │ None │ 2 │ 3 │ 4 │ 5 │ 6 │ 7 │
  ╞═════════════╪══════════╪══════════╪══════════╪══════════╪══════════╪══════════╪═════════╡
  │ Accuracy │ 0.918919 │ 0.472973 │ 0.554054 │ 0.621622 │ 0.621622 │ 0.675676 │ 0.72973 │
  ╘═════════════╧══════════╧══════════╧══════════╧══════════╧══════════╧══════════╧═════════╛

- I've written a short essay providing my insights on the statistics like this "Overall, the accuracy of the model is positively correlated with the maximum depth of the tree.

When set at just two levels, the classification model exhibit an extremely poor performance of approximately 0.47, which is totally unreliable, especially when it comes to a medical diagnosisic problem like this. Fortunately, the more deeply the tree is allowed to span, the higher the accuracy gets. For the later higher maximum depth, the figure gradually gets better and better, ultimately it peaks at 0.91 when there is no limit on the tree's depth.

However, one particularly odd characteristic is that the accuracy reaches a plateau when we shift the tree maximum depth from three to four, which is not the case for other depth increases. This can boil down to the fact that the fourth level added only helps to clarify some tanglement which unfortunately do not directly constitute towards the general accuracy of the predictions.

Nevetheless, the less restriction imposed on the tree depth, the more complex and diverse the branches become, which fosters more thorough features study before making the final prediction. This proves to be incredibly crucial in recognizing the pattern as this dataset possesses over thirty different input features with many common points between classes.

In general, the model's accuracy grows steadily with the tree maximum depth. This can be attributed to the fact that it needs many branches and nodes to accommodate the complexity of the overlapping features of the six classes."
Please polish it to be more coherent and concise; also is there any information I can derive from the stats?
-> answer:
Additional insights you can mention:
Diminishing returns: While accuracy increases with depth, the gains between consecutive depth levels shrink after depth 4. This suggests diminishing returns, and could guide pruning or early stopping decisions.

Overfitting risk: Although depth=None gives the highest accuracy, it may overfit the training data. A validation curve or cross-validation analysis could help confirm this.

Sweet spot identification: A depth of 6 or 7 may offer a good trade-off between complexity and performance, especially if runtime or interpretability is a concern.

Baseline comparison: With random guessing across 6 classes yielding roughly 16.7% accuracy, even depth-2 trees outperform chance—though not by much. This emphasizes how critical model capacity is to effective classification.

- please add in the baseline comparison in the paragraph where depth of two is first mentioned, saying that it's better than wild guess, but still not enough, especially when it's about medical diagnostics, and emphasizes model capacity

# Reference

- https://vitalflux.com/pandas-impute-missing-values-mean-median-mode/ (17/6/2025)
- https://pressbooks-dev.oer.hawaii.edu/introductorystatistics/chapter/skewness-and-the-mean-median-and-mode/ (20/6/2025)
- https://scikit-learn.org/stable/modules/impute.html (20/6/2025)
- https://www.statology.org/sklearn-classification-report/ (23/6/2025)
- https://www.evidentlyai.com/classification-metrics/accuracy-precision-recall (23/6/2025)
- https://www.evidentlyai.com/classification-metrics/confusion-matrix (24/6/2025)
